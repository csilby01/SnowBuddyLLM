{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import httplib2\n",
    "import json\n",
    "import requests\n",
    "from yt_dlp import YoutubeDL\n",
    "import scrapetube\n",
    "from IPython.display import clear_output\n",
    "import evaluate\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems to solve:\n",
    "- How to collect URLs from current page\n",
    "- How to collect URLs from different pages\n",
    "- How to collect information from pages within each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url):\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(url)\n",
    "    return response\n",
    "\n",
    "def filterProductLinks(response, category):\n",
    "    #filter out all links from HTML response\n",
    "    soup = BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a'))\n",
    "\n",
    "    links=[]\n",
    "    #filtering by all links that are product pages we want\n",
    "    for link in soup:\n",
    "        if link.has_attr('href') and category in link['href'] and \"shop\" not in link['href']:\n",
    "            links.append(link['href'])\n",
    "    return links\n",
    "\n",
    "def collectURLs(main_url, category):\n",
    "    i = 1\n",
    "    links = []\n",
    "    while True:\n",
    "        #Moves through the pages in the store as needed\n",
    "        temp_url = main_url + \"/p_{}\".format(i) if i != 1 else main_url\n",
    "\n",
    "        #Collects all present URLs on page with BeautifulSoup library    \n",
    "        response = getPage(temp_url)\n",
    "        new_links = filterProductLinks(response, category)\n",
    "\n",
    "        #If there are no product links, we are complete\n",
    "        if not new_links:\n",
    "            break\n",
    "\n",
    "        links.extend(new_links)\n",
    "        #Go to next page\n",
    "        i+=1\n",
    "    return links\n",
    "\n",
    "def getNameDescription(soup):\n",
    "    text, product_name, old_string, to_append = \"\", \"\", \"\", 0\n",
    "    for i, string in enumerate(soup.stripped_strings):\n",
    "        #the previous line before sku# is AlWAYS the product's name\n",
    "        if \"sku#\" in string:\n",
    "            product_name = old_string\n",
    "        #Once we see \"Specs\", we are no longer interested in the info on the page\n",
    "        if string == \"Specs\" or string == \"Effective Edge (mm)\":\n",
    "            to_append = 0\n",
    "        #if info is in the product details, we append to our main text\n",
    "        if to_append == 1:\n",
    "            text = text + \" \" + string\n",
    "        #We want to search for everything in the product details section\n",
    "        if string == \"Product Details\":\n",
    "            to_append = 1\n",
    "        #overwrite before going to the next line\n",
    "        old_string = string\n",
    "    return product_name, text\n",
    "\n",
    "def collectProductInfo(links, main_url):\n",
    "    products = []\n",
    "    for i, link in enumerate(links):\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Current progress: {i}/{len(links)} products processed.\")\n",
    "        combined_url = main_url + link\n",
    "        #get html\n",
    "        response = getPage(combined_url)\n",
    "\n",
    "        #parse plaintext\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "        #collect name and description of product and append to product list\n",
    "        product_name, text = getNameDescription(soup)\n",
    "        products.append({'name': product_name, 'description': text.strip()})\n",
    "    return products\n",
    "\n",
    "def jsonDump(filename, contents):\n",
    "    with open(f\"{filename}.json\", \"w\") as f:\n",
    "        json.dump(contents, f, indent=2)\n",
    "    return True\n",
    "\n",
    "def createProductJSON():\n",
    "    #Collect the links to each product page\n",
    "    links = collectURLs(\"https://www.evo.com/shop/snowboard/snowboards/condition_new\", \"snowboards/\") + collectURLs(\"https://www.evo.com/shop/snowboard/bindings/condition_new\", \"bindings/\")\n",
    "    \n",
    "    #Next, we will collect the text from every single product page that we are interested in\n",
    "    products = collectProductInfo(links, \"https://www.evo.com\")\n",
    "\n",
    "    #Create json file\n",
    "    jsonDump(\"products\", products)\n",
    "    clear_output(wait=True)\n",
    "    print(\"JSON file successfully created\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current progress: 700/726 products processed.\n",
      "JSON file successfully created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createProductJSON()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
