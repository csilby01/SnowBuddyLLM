{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, DataCollatorForLanguageModeling, Gemma3ForCausalLM, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig,AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType, get_peft_model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# trl: Transformer Reinforcement Learning library\n",
    "\n",
    "# from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "# from trl import create_reference_model\n",
    "# from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d97f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# Inference pipeline\n",
    "chat = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM #GEMMA\n",
    ")\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "peft_model = get_peft_model(model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))\n",
    "\n",
    "peft_model_base = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, device_map = \"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       \"./peft-snowboard-llm-checkpoint-local\", \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)\n",
    "peft_model.eval()\n",
    "chat_ft = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"hi\"\n",
    "response = chat_ft(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)[0]['generated_text']\n",
    "response = response.replace(prompt, \"\").strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d255da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages(sample):\n",
    "    messages = sample[\"messages\"]\n",
    "    prompt = \"\"\n",
    "    for m in messages:\n",
    "        role = m[\"role\"]\n",
    "        content = m[\"content\"]\n",
    "        if role == \"system\":\n",
    "            prompt += f\"<|system|>\\n{content}\\n\"\n",
    "        elif role == \"user\":\n",
    "            prompt += f\"<|user|>\\n{content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>\\n{content}\\n\"\n",
    "    return {\"text\": prompt.strip()}\n",
    "def tokenize(sample):\n",
    "    result = tokenizer(\n",
    "        sample[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1000,\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files = [\"snowboard_data.json\", \"binding_data.json\"])\n",
    "formatted_data = training_data.map(format_messages)\n",
    "formatted_data = formatted_data.map(tokenize, remove_columns=formatted_data.column_names)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "output_dir = f'./peft-snowboard-info-training-{str(int(time.time()))}'\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,  \n",
    "    gradient_accumulation_steps=16, \n",
    "    learning_rate=3e-4,            \n",
    "    weight_decay=0.01,             \n",
    "    warmup_ratio=0.1, \n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps = 500,\n",
    "    remove_unused_columns=False,\n",
    "    save_total_limit=2 \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=formatted_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-snowboard-llm-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
